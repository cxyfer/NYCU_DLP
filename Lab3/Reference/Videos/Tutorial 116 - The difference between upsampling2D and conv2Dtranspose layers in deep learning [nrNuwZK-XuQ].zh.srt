1
00:00:00,880 --> 00:00:06,560
嗨大家，歡迎來到介紹性 Python 教學，特別關注於影像處理

2
00:00:06,560 --> 00:00:12,240
在這段影片中，我們來快速了解一下升採樣和卷積的區別

3
00:00:12,240 --> 00:00:14,080
和卷積

4
00:00:14,080 --> 00:00:15,200
轉置操作

5
00:00:15,200 --> 00:00:17,760
而且這兩者都可以是 2D

6
00:00:17,760 --> 00:00:18,720
和 3D

7
00:00:18,720 --> 00:00:22,160
而且這些通常會在單位中使用

8
00:00:22,160 --> 00:00:24,960
以及其他架構，如自編碼器

9
00:00:24,960 --> 00:00:28,080
以及生成對抗網路

10
00:00:28,080 --> 00:00:32,159
如果你不知道我們在說什麼單位，我強烈推薦

11
00:00:32,159 --> 00:00:34,320
觀看最後兩三部影片

12
00:00:34,320 --> 00:00:36,160
我們討論了自動編碼器

13
00:00:36,160 --> 00:00:41,840
以及單位與自動編碼器非常相似，只是跳過連接部分不同

14
00:00:41,840 --> 00:00:45,440
現在在解碼器部分

15
00:00:45,440 --> 00:00:50,000
這意味著在機器學習中你會有一個較小的向量

16
00:00:50,000 --> 00:00:53,120
在編碼器部分，你是從一個大型圖像轉換到一個較小的向量

17
00:00:53,120 --> 00:00:55,600
以及解碼器部分，您從一個小的

18
00:00:55,600 --> 00:00:59,280
張量，我應該說是從較小的張量回到大型圖像

19
00:00:59,280 --> 00:01:02,160
同時，您是如何升級您的

20
00:01:02,160 --> 00:01:05,600
或者提升您的影像解析度

21
00:01:05,600 --> 00:01:06,400
所以

22
00:01:06,400 --> 00:01:07,439
從較小的

23
00:01:07,439 --> 00:01:11,040
尺寸到較大的尺寸，這就是我們所說的，這裡有兩個選擇，主要的

24
00:01:11,040 --> 00:01:12,960
我應該說其中一個是升頻

25
00:01:12,960 --> 00:01:15,439
你可以根據名稱升頻你的影像

26
00:01:15,439 --> 00:01:16,960
這意味著你正在取樣

27
00:01:16,960 --> 00:01:18,560
獲取像素值

28
00:01:18,560 --> 00:01:21,439
你正在以某種方式擴展它們

29
00:01:21,439 --> 00:01:24,080
你也可以進行轉置操作

30
00:01:24,080 --> 00:01:27,759
首先讓我們看看這兩者之間的差異

31
00:01:27,759 --> 00:01:32,000
再次提醒你，我們討論的單位是編碼器路徑，對吧

32
00:01:32,000 --> 00:01:32,960
所以

33
00:01:32,960 --> 00:01:38,400
為什麼尺寸會隨著向下移動而變小，那是因為當你應用

34
00:01:38,400 --> 00:01:42,399
最大池化操作，大小為兩乘兩，對吧，那正是

35
00:01:42,399 --> 00:01:44,640
我們用這個紅色箭頭做了什麼

36
00:01:44,640 --> 00:01:47,520
這會將你的圖像大小減少一半

37
00:01:47,520 --> 00:01:52,159
所以你從 256 變成 128，再變成 64，再變成 32，最後變成 16。

38
00:01:52,159 --> 00:01:56,399
在解碼器端，我們需要做完全相反的操作，從 16 轉換到 32

39
00:01:56,399 --> 00:02:01,119
如此類推，這是我們實際上可以應用我們的 2D 上採樣的地方

40
00:02:01,119 --> 00:02:02,880
或者轉換 2D 轉置

41
00:02:02,880 --> 00:02:06,000
現在讓我們更仔細地看一下，既然我們知道了背景

42
00:02:06,000 --> 00:02:09,038
現在讓我們更仔細地看一下這些

43
00:02:09,038 --> 00:02:09,840
所以

44
00:02:09,840 --> 00:02:10,720
有兩種類型

45
00:02:10,720 --> 00:02:12,480
像我們之前提到的那樣

46
00:02:12,480 --> 00:02:13,440
可以使用

47
00:02:13,440 --> 00:02:17,040
上採樣 2d 非常類似於

48
00:02:17,040 --> 00:02:20,640
最大池化，除了它顯然與最大池化相反

49
00:02:20,640 --> 00:02:24,239
在 2D 上採樣中，我們會重複行

50
00:02:24,239 --> 00:02:27,040
和輸入的列

51
00:02:27,040 --> 00:02:27,360
我會

52
00:02:27,360 --> 00:02:31,280
我會以視覺化和程式化的方式解釋這一點

53
00:02:31,280 --> 00:02:35,440
轉換 2D 轉置，顧名思義是一種卷積操作

54
00:02:35,440 --> 00:02:40,560
好的，所以這裡有一個乘法運算，上採樣只是重複數據

55
00:02:40,560 --> 00:02:41,840
所以如果你有

56
00:02:41,840 --> 00:02:45,920
如果你有，我們稍後會討論到，不過讓我先不

57
00:02:45,920 --> 00:02:48,640
你知道，嘗試提供你一些抽象的細節

58
00:02:48,640 --> 00:02:51,599
但我們會有一個視覺解釋

59
00:02:51,599 --> 00:02:53,840
並且不能進行 2D 轉置

60
00:02:53,840 --> 00:02:54,239
如果你

61
00:02:54,239 --> 00:02:57,360
如果你想弄清楚使用哪一個

62
00:02:57,360 --> 00:03:00,879
我個人沒有看到太多差異

63
00:03:00,879 --> 00:03:02,319
使用這兩者中的任何一個

64
00:03:02,319 --> 00:03:07,360
但數量轉置已被報告會導致棋盤格伪影

65
00:03:07,360 --> 00:03:07,680
好的

66
00:03:07,680 --> 00:03:09,280
但比較不多

67
00:03:09,280 --> 00:03:14,159
這兩者之間的文獻可能不多，也許我漏掉了什麼

68
00:03:14,159 --> 00:03:15,519
如果你們知道

69
00:03:15,519 --> 00:03:19,360
任何這兩者之間的比較論文，請在評論中留下

70
00:03:19,360 --> 00:03:26,959
但對我而言，兩者都同樣好，我想更喜歡

71
00:03:26,959 --> 00:03:28,239
2D 轉置

72
00:03:28,239 --> 00:03:34,159
因為這樣你也會學到一些，你知道，機密的轉置實際上會學到

73
00:03:34,159 --> 00:03:36,720
和上採樣那裡沒有東西可以學習

74
00:03:36,720 --> 00:03:42,319
希望在接下來的五到十分鐘內，事情能變得更清楚一些

75
00:03:42,319 --> 00:03:45,680
那麼讓我先談談上採樣

76
00:03:45,680 --> 00:03:46,159
好的

77
00:03:46,159 --> 00:03:50,000
假設這是我的輸入圖像，它只是一個三乘三像素的圖像

78
00:03:50,000 --> 00:03:50,799
好的

79
00:03:50,799 --> 00:03:54,879
現在我想進行二比二的上採樣

80
00:03:54,879 --> 00:03:56,959
所以這就是

81
00:03:56,959 --> 00:03:58,239
基本上

82
00:03:58,239 --> 00:04:01,599
重複你的像素

83
00:04:01,599 --> 00:04:06,159
兩倍兩倍，所以在這種情況下，我的一張圖片就是我一個像素，抱歉

84
00:04:06,159 --> 00:04:07,200
這個紅色

85
00:04:07,200 --> 00:04:09,200
正方形是四個紅色正方形

86
00:04:09,200 --> 00:04:11,280
然後是四個粉紅色的

87
00:04:11,280 --> 00:04:13,120
然後是黃色的，以此類推

88
00:04:13,120 --> 00:04:15,120
這樣你就可以在這裡看到圖像

89
00:04:15,120 --> 00:04:18,560
是一二三四五六對六乘六

90
00:04:18,560 --> 00:04:20,560
我們從三乘三開始

91
00:04:20,560 --> 00:04:22,800
所以尺寸翻倍了

92
00:04:22,800 --> 00:04:25,120
因為我們使用的是二比二的上採樣

93
00:04:25,120 --> 00:04:30,639
這正是為什麼我們在 unit decoder 中，我們從 16 變成 32 再到 64

94
00:04:30,639 --> 00:04:32,400
因此大小會加倍

95
00:04:32,400 --> 00:04:34,800
如果你進行兩兩操作

96
00:04:34,800 --> 00:04:39,759
因此上採樣容易理解，你只是複製，這裡沒有任何需要訓練的東西

97
00:04:39,759 --> 00:04:44,400
你拿一個矩陣，你拿像數字這樣的東西，當涉及到卷積時，你會重複它們

98
00:04:44,400 --> 00:04:47,520
再次進行 2D 轉置，讓我們使用相同的輸入

99
00:04:47,520 --> 00:04:50,560
這樣可以讓我們更容易理解

100
00:04:50,560 --> 00:04:55,680
這會轉換成看起來像這樣的東西，你知道的

101
00:04:55,680 --> 00:04:56,720
讓我解釋一下

102
00:04:56,720 --> 00:04:59,199
這是一個卷積操作

103
00:04:59,199 --> 00:05:00,160
所以我們需要定義

104
00:05:00,160 --> 00:05:02,720
內核大小將會是什麼

105
00:05:02,720 --> 00:05:07,440
在這個例子中，我的內核大小是 1x1，就是 1x1

106
00:05:07,440 --> 00:05:10,560
但是我的步幅是 2x2，這意味著我每次移動兩個單位

107
00:05:10,560 --> 00:05:13,919
每次一步，所以那裡有間距

108
00:05:13,919 --> 00:05:16,960
在這兩者之間是可以的，因為我的步幅是二乘二

109
00:05:16,960 --> 00:05:19,919
所以把這個想像成非常類似於二乘二的上採樣

110
00:05:19,919 --> 00:05:22,160
那裡正在創建這個黑暗的空間

111
00:05:22,160 --> 00:05:24,880
但它沒有填充任何東西

112
00:05:24,880 --> 00:05:29,440
這裡它用這些像素值進行上採樣

113
00:05:29,440 --> 00:05:31,520
這裡它沒有填充

114
00:05:31,520 --> 00:05:33,600
就是那裡的資訊

115
00:05:33,600 --> 00:05:35,120
現在

116
00:05:35,120 --> 00:05:41,280
因為這是卷積操作，所以還有一些稱為權重的東西你可以定義

117
00:05:41,280 --> 00:05:46,320
什麼是權重，權重基本上是卷積內的值

118
00:05:46,320 --> 00:05:49,440
在這個例子中，大小為一對一的濾波器

119
00:05:49,440 --> 00:05:53,039
所以我要設置一個權重為 1，這意味著我的輸出

120
00:05:53,039 --> 00:05:56,560
將會具有與我的輸入相同的值

121
00:05:56,560 --> 00:05:58,080
這裡有這個值

122
00:05:58,080 --> 00:06:01,919
我希望這樣解釋能讓你明白，我們會寫一行

123
00:06:01,919 --> 00:06:04,800
或者幾行來更好地理解這一點

124
00:06:04,800 --> 00:06:07,440
但我只想確保在視覺上

125
00:06:07,440 --> 00:06:09,120
我們在同一頁面上

126
00:06:09,120 --> 00:06:12,000
上採樣非常簡單對吧

127
00:06:12,000 --> 00:06:15,280
兩兩進行上採樣，就是複製這個卷積

128
00:06:15,280 --> 00:06:18,720
如果你需要了解什麼是卷積，請觀看關於卷積的視頻

129
00:06:18,720 --> 00:06:20,560
這裡的卷積是你有你的圖像

130
00:06:20,560 --> 00:06:22,160
和你有一個核

131
00:06:22,160 --> 00:06:25,360
你在影像的每個像素上進行卷積運算

132
00:06:25,360 --> 00:06:26,880
並且你正在移動那個卷積核

133
00:06:26,880 --> 00:06:29,600
在這個情況下，我的卷積核大小只有一個

134
00:06:29,600 --> 00:06:30,720
或一個接一個

135
00:06:30,720 --> 00:06:34,400
而且我每次移動兩步，這就是為什麼你有

136
00:06:34,400 --> 00:06:36,880
從這裡開始，它會走一、二步

137
00:06:36,880 --> 00:06:37,600
一 二

138
00:06:37,600 --> 00:06:42,560
然後在這個方向上，一二，這裡就是你擁有這個空間的地方

139
00:06:42,560 --> 00:06:46,000
好的，現在這個空間被填滿了所有的零值

140
00:06:46,000 --> 00:06:50,639
但是隨著學習，這個空間可以被填充新的值，這基本上就是

141
00:06:50,639 --> 00:06:52,880
什麼是 2d 轉置

142
00:06:52,880 --> 00:06:53,360
好的

143
00:06:53,360 --> 00:06:57,840
現在讓我們使用簡單的 Python 代碼來更好地理解這些差異

144
00:06:57,840 --> 00:07:00,720
那麼讓我們跳到我們的 Collab

145
00:07:00,720 --> 00:07:02,160
這段代碼我將與你分享

146
00:07:02,160 --> 00:07:03,520
所以請注意

147
00:07:03,520 --> 00:07:06,160
我們現在正在討論的內容

148
00:07:06,160 --> 00:07:09,360
首先，我們來演示 2D 上採樣

149
00:07:09,360 --> 00:07:11,840
然後我們可以進行 2D 轉置

150
00:07:11,840 --> 00:07:14,240
為此，我將導入

151
00:07:14,240 --> 00:07:15,599
TensorFlow 序列

152
00:07:15,599 --> 00:07:18,560
所以我們會建立一個小型模型

153
00:07:18,560 --> 00:07:20,160
只有上採樣層

154
00:07:20,160 --> 00:07:20,639
好的

155
00:07:20,639 --> 00:07:22,720
所以這就是為什麼我們要進行上採樣

156
00:07:22,720 --> 00:07:23,840
當然還有輸入

157
00:07:23,840 --> 00:07:25,360
這樣我們就可以定義輸入

158
00:07:25,360 --> 00:07:27,360
好的，我們的數據是什麼

159
00:07:27,360 --> 00:07:29,039
通常我們處理的是影像

160
00:07:29,039 --> 00:07:31,520
但在這種情況下，我們想要確切了解

161
00:07:31,520 --> 00:07:37,039
什麼是取樣，讓我們定義一個三乘三像素的圖像

162
00:07:37,039 --> 00:07:38,400
我填充了我們的圖像

163
00:07:38,400 --> 00:07:41,840
或 numpy 陣列，值為一、二、三、四、五、六、七、八、九

164
00:07:41,840 --> 00:07:44,000
這樣我們可以弄清楚是否有重複的內容

165
00:07:44,000 --> 00:07:44,479
好的

166
00:07:44,479 --> 00:07:46,000
所以這是我的 x

167
00:07:46,000 --> 00:07:48,639
這是我目前的圖像

168
00:07:48,639 --> 00:07:51,520
什麼是影像？它只是 numpy 陣列對吧

169
00:07:51,520 --> 00:07:53,199
現在我要重塑它

170
00:07:53,199 --> 00:07:56,160
這樣它就可以以正確的形狀進入我的神經網絡

171
00:07:56,160 --> 00:07:58,160
所以我們來執行這一行

172
00:07:58,160 --> 00:08:00,000
然後沿著這條線

173
00:08:00,000 --> 00:08:03,199
所以我的形狀基本上是處理我的三乘三輸入

174
00:08:03,199 --> 00:08:05,919
我有一張三乘三大小的圖片

175
00:08:05,919 --> 00:08:06,960
它是單通道的

176
00:08:06,960 --> 00:08:08,319
對，我沒有很多通道

177
00:08:08,319 --> 00:08:10,080
就在那裡，那就是輸入的內容

178
00:08:10,080 --> 00:08:12,960
這已經準備好進入我的神經網絡，一旦我定義好

179
00:08:12,960 --> 00:08:15,360
那麼我的神經網絡是什麼

180
00:08:15,360 --> 00:08:16,240
或我的模型

181
00:08:16,240 --> 00:08:18,560
那麼我將使用順序方法

182
00:08:18,560 --> 00:08:20,560
而我的輸入尺寸為這個

183
00:08:20,560 --> 00:08:25,680
這是三乘三乘一的大小，這是我的輸入

184
00:08:25,680 --> 00:08:26,240
好的

185
00:08:26,240 --> 00:08:32,000
然後我將添加一個大小為二乘二的上採樣層

186
00:08:32,000 --> 00:08:36,000
我們已經從圖像中看到，如果我們有

187
00:08:36,000 --> 00:08:38,000
二乘二的上採樣

188
00:08:38,000 --> 00:08:40,000
或圖像尺寸加倍

189
00:08:40,000 --> 00:08:42,000
那麼我會期望的

190
00:08:42,000 --> 00:08:48,000
因此從這裡的結果是一六六一，它會加倍對吧

191
00:08:48,000 --> 00:08:51,120
每一個

192
00:08:51,120 --> 00:08:52,880
所以讓我們繼續定義這個

193
00:08:52,880 --> 00:08:54,959
我們馬上就會看到結果

194
00:08:54,959 --> 00:08:57,920
所以這是我的模型摘要

195
00:08:57,920 --> 00:09:02,160
你可以看到摘要已經顯示它將給我一個輸出

196
00:09:02,160 --> 00:09:07,600
一張影像，這裡的“無”意味著我們有一張六乘六的單通道影像

197
00:09:07,600 --> 00:09:13,040
和有多少個參數來訓練零，沒有任何東西

198
00:09:13,040 --> 00:09:16,399
讓我重複一次，這裡沒有需要訓練的東西

199
00:09:16,399 --> 00:09:16,800
好的

200
00:09:16,800 --> 00:09:20,240
因此，值為零，訓練參數的數量

201
00:09:20,240 --> 00:09:23,360
因此，如果你擔心增加可訓練參數的數量

202
00:09:23,360 --> 00:09:28,880
然後如果你的網路變慢，你可以繼續使用上採樣

203
00:09:28,880 --> 00:09:33,200
但始終記住，上採樣作為unit的一部分是會被應用的

204
00:09:33,200 --> 00:09:35,200
在卷積操作之後

205
00:09:35,200 --> 00:09:38,880
所以這並不是說你會大幅增加它

206
00:09:38,880 --> 00:09:41,760
使用卷積 2d 轉置，你擁有卷積

207
00:09:41,760 --> 00:09:43,600
和轉置類似的內建功能

208
00:09:43,600 --> 00:09:45,040
所以

209
00:09:45,040 --> 00:09:50,399
所以你仍然需要訓練幾乎相同數量的

210
00:09:50,399 --> 00:09:52,000
參數

211
00:09:52,000 --> 00:09:52,640
好的

212
00:09:52,640 --> 00:09:57,920
到目前為止我們什麼都沒做，我們只是將我們的影像定義為三乘三的矩陣，並重新塑造它

213
00:09:57,920 --> 00:10:00,560
我們定義了我們的模型為接收一些輸入

214
00:10:00,560 --> 00:10:04,480
然後是上採樣層，讓我們將這個模型應用到我們的圖像上

215
00:10:04,480 --> 00:10:08,000
你怎麼做到這點？其實就是使用 model.predict 對吧

216
00:10:08,000 --> 00:10:12,160
所以我們使用這個模型對原始影像進行預測

217
00:10:12,160 --> 00:10:13,600
所以 x 是我們的輸入

218
00:10:13,600 --> 00:10:15,600
所以當你這樣做時，我們繼續

219
00:10:15,600 --> 00:10:17,600
並列印出來

220
00:10:17,600 --> 00:10:21,680
原始輸入是 一 二 三 四 五 六 七 八 九 我們知道

221
00:10:21,680 --> 00:10:22,720
和上採樣的

222
00:10:22,720 --> 00:10:25,200
這是來自的輸出

223
00:10:25,200 --> 00:10:30,880
或者來自我們的 model.predict 是一一二二三三一一二二三三和

224
00:10:30,880 --> 00:10:35,600
四四五五六六，這與我展示給你的紅色粉紅色黃色盒子非常相似

225
00:10:35,600 --> 00:10:39,600
每一個，如果其中一個是紅色盒子，它會重複四次

226
00:10:39,600 --> 00:10:42,560
這是上採樣，就這樣，所以這就是我的做法

227
00:10:42,560 --> 00:10:46,000
從較小的圖像到較大的尺寸

228
00:10:46,000 --> 00:10:49,040
正如你所見，這實際上不是一個很好的方法

229
00:10:49,040 --> 00:10:52,560
使你的原始影像解析度恢復正確

230
00:10:52,560 --> 00:10:52,959
我的意思是

231
00:10:52,959 --> 00:10:55,760
你在這裡失去了影像解析度

232
00:10:55,760 --> 00:10:59,600
這正是為什麼在 unit 中連接非常重要

233
00:10:59,600 --> 00:11:01,519
從早期的卷積網絡

234
00:11:01,519 --> 00:11:05,120
encoder 層卷積層到 decoder 層

235
00:11:05,120 --> 00:11:05,760
好的

236
00:11:05,760 --> 00:11:08,480
現在讓我們看看 2d 轉置實際上是如何工作的

237
00:11:08,480 --> 00:11:09,120
好的

238
00:11:09,120 --> 00:11:11,040
那麼我們來做一個類似的

239
00:11:11,040 --> 00:11:15,760
幾乎完全相同的例子，只是這次我不是進行上採樣，而是將 con 進口到 d 轉置

240
00:11:15,760 --> 00:11:19,040
好的，所以其他的完全相同

241
00:11:19,040 --> 00:11:24,480
然後在我的模型中，我將使用 con 2d transpose 來代替上採樣

242
00:11:24,480 --> 00:11:27,279
讓我們使用我之前給你的相同示例

243
00:11:27,279 --> 00:11:32,320
即一乘一的卷積核大小和二乘二的步幅

244
00:11:32,320 --> 00:11:35,200
現在這一點很重要，當我設置內核初始化器時

245
00:11:35,200 --> 00:11:38,079
等於一次，這意味著它初始化

246
00:11:38,079 --> 00:11:41,360
我內核中的所有值為一

247
00:11:41,360 --> 00:11:44,399
這等同於重量等於 1

248
00:11:44,399 --> 00:11:50,800
這意味著如果我有這樣的輸入圖像，它會將每個值乘以 1

249
00:11:50,800 --> 00:11:55,600
所以我們的值不會改變，我們將再次看到這個效果，我之前談到過

250
00:11:55,600 --> 00:11:58,160
視頻中的核心初始化器

251
00:11:58,160 --> 00:12:00,959
如果你還沒看過那個，我強烈推薦你去看看

252
00:12:00,959 --> 00:12:05,600
如果你不定義這個，那麼 Keras 將使用默認值

253
00:12:05,600 --> 00:12:07,040
初始化權重的方式

254
00:12:07,040 --> 00:12:10,639
我相信這是一種常態分佈隨機分佈

255
00:12:10,639 --> 00:12:14,240
那麼，隨機初始化會有一些變異

256
00:12:14,240 --> 00:12:14,959
好的

257
00:12:14,959 --> 00:12:18,079
那麼我們來定義這個模型

258
00:12:18,079 --> 00:12:21,120
和一件事，好，這很重要

259
00:12:21,120 --> 00:12:24,240
現在我的步幅是二乘二

260
00:12:24,240 --> 00:12:26,639
而我的卷積核大小是 一乘一，對吧

261
00:12:26,639 --> 00:12:27,600
所以

262
00:12:27,600 --> 00:12:30,720
這是總結，我的輸出將會是

263
00:12:30,720 --> 00:12:34,079
六乘六乘一的尺寸，完全相同的大小

264
00:12:34,079 --> 00:12:37,279
就像我們如果放大它的話

265
00:12:37,279 --> 00:12:38,320
完全相同的大小

266
00:12:38,320 --> 00:12:39,360
因為

267
00:12:39,360 --> 00:12:42,480
再次，步幅是，對不起，我們的步幅是兩步兩步，那正是

268
00:12:42,480 --> 00:12:44,000
為什麼我們有六六一

269
00:12:44,000 --> 00:12:47,920
但這裡的關鍵點是查看被訓練的參數數量

270
00:12:47,920 --> 00:12:51,440
作為我們神經網絡訓練的一部分可訓練

271
00:12:51,440 --> 00:12:55,200
這是兩個參數，為什麼要一一查看內核大小

272
00:12:55,200 --> 00:12:57,360
這意味著只有一個值

273
00:12:57,360 --> 00:13:00,560
這意味著它正在嘗試學習，這是權重

274
00:13:00,560 --> 00:13:05,360
另一個是偏差，記住人工神經元總是有一個權重

275
00:13:05,360 --> 00:13:06,560
和一個偏差對吧

276
00:13:06,560 --> 00:13:11,040
如果有一個神經元，那麼你有兩個可訓練的參數，一個權重

277
00:13:11,040 --> 00:13:17,120
而且這正是為什麼你有那裡的兩個參數，如果我改變我的核心

278
00:13:17,120 --> 00:13:21,760
我認為這是一個非常有教育意義的練習，對不起，兩兩來做這個

279
00:13:21,760 --> 00:13:23,200
然後再次運行這個

280
00:13:23,200 --> 00:13:27,040
我的參數數量將是五個，仍然是六個六

281
00:13:27,040 --> 00:13:29,360
因為我的步伐是兩兩一組

282
00:13:29,360 --> 00:13:33,839
好的，但可訓練的參數數量是五，為什麼

283
00:13:33,839 --> 00:13:35,440
因為如果是兩兩一組

284
00:13:35,440 --> 00:13:36,560
我有多少個

285
00:13:36,560 --> 00:13:39,839
多少個權重，四個，對，兩兩一組

286
00:13:39,839 --> 00:13:43,279
所以我有四個權重，網絡需要訓練

287
00:13:43,279 --> 00:13:46,160
在訓練過程中加上一個偏差

288
00:13:46,160 --> 00:13:46,959
為了它

289
00:13:46,959 --> 00:13:47,279
好的

290
00:13:47,279 --> 00:13:50,240
所以你有四加一等於五，這就是為什麼你有五

291
00:13:50,240 --> 00:13:52,720
所以我猜也許你知道

292
00:13:52,720 --> 00:13:56,399
當你使用三乘三的核時會發生什麼

293
00:13:56,399 --> 00:13:59,680
好的，正確

294
00:14:00,160 --> 00:14:05,360
如果你有三乘三，那就是九對吧，九加一，十個參數

295
00:14:05,360 --> 00:14:08,240
那麼我們回到逐一處理

296
00:14:08,240 --> 00:14:09,839
逐一處理是我們想要的

297
00:14:09,839 --> 00:14:13,279
那麼我們繼續進行應用，好的，這就是我們所在的位置

298
00:14:13,279 --> 00:14:14,079
現在

299
00:14:14,079 --> 00:14:15,920
讓我們應用這個模型 dot predict

300
00:14:15,920 --> 00:14:19,680
我稱這個模型為 one right，model one dot predict 在我們的輸入數據上

301
00:14:19,680 --> 00:14:22,880
現在我們來印出所有轉置過的數據

302
00:14:22,880 --> 00:14:24,639
以及那些上採樣值

303
00:14:24,639 --> 00:14:25,680
所以

304
00:14:25,680 --> 00:14:26,959
我的原始輸入

305
00:14:26,959 --> 00:14:28,480
和轉置的

306
00:14:28,480 --> 00:14:31,199
卷積未

307
00:14:42,959 --> 00:14:45,440
由於步幅為一，因此不會有任何零值

308
00:14:45,440 --> 00:14:48,480
和你有值讓我們

309
00:14:48,480 --> 00:14:50,480
一個一個做

310
00:14:50,480 --> 00:14:55,600
所以當你一個一個做時，這意味著你的輸出形狀也是三個

311
00:14:55,600 --> 00:14:57,279
同原始輸入相同

312
00:14:57,279 --> 00:15:02,320
所以那裡沒有任何變化，我們也做另一件事

313
00:15:02,320 --> 00:15:05,760
我們來移除內核初始化部分

314
00:15:05,760 --> 00:15:06,320
和查看

315
00:15:06,320 --> 00:15:10,720
輸出結果應該是這樣的，這部分應該完全一樣，我們不會改變任何東西

316
00:15:10,720 --> 00:15:15,360
但當我們將它應用到圖像上時，我們會在這裡看到一些奇怪的數字

317
00:15:15,360 --> 00:15:17,040
為什麼我們會看到這些奇怪的數字

318
00:15:17,040 --> 00:15:20,160
因為這是隨機初始化的

319
00:15:20,160 --> 00:15:22,000
權重不再是 1

320
00:15:22,000 --> 00:15:25,680
所以顯然權重變成了 0.96

321
00:15:25,680 --> 00:15:26,560
在該位置

322
00:15:26,560 --> 00:15:29,199
所以 1 乘以 0.96 等於 0.96

323
00:15:29,199 --> 00:15:32,160
這裡的重量大約是 0.8

324
00:15:32,160 --> 00:15:35,600
這裡得到的結果是某個東西乘以 2

325
00:15:35,600 --> 00:15:36,079
好的

326
00:15:36,079 --> 00:15:39,600
所以你的權重在 0 的範圍內隨機生成

327
00:15:39,600 --> 00:15:40,399
和 1

328
00:15:40,399 --> 00:15:43,199
然後乘以你這裡擁有的數字

329
00:15:43,199 --> 00:15:46,720
然後你會得到這個，這就是為什麼 con 2d 我希望

330
00:15:46,720 --> 00:15:51,680
我希望這解釋了 con 2d，這裡我通過一次初始化內核

331
00:15:51,680 --> 00:15:52,959
所以

332
00:15:52,959 --> 00:15:58,079
你的輸出將會和輸入一模一樣，只是多了一些空格

333
00:15:58,079 --> 00:16:01,360
因為我們只是把這些乘以一

334
00:16:01,360 --> 00:16:03,519
非常好的練習，尤其是

335
00:16:03,519 --> 00:16:07,920
如果您對這個神經網絡主題不熟悉，這些小細節將

336
00:16:07,920 --> 00:16:14,000
當您執行內核初始化程序時，這些小細節將真正提供有關發生什麼事的深入了解

337
00:16:14,000 --> 00:16:16,480
等於正常，例如

338
00:16:16,480 --> 00:16:17,680
我覺得或嘿，正常

339
00:16:17,680 --> 00:16:20,560
如果你是那個 hg

340
00:16:20,560 --> 00:16:23,920
嘿，正常，我覺得那就是你拼寫的方式，是的

341
00:16:23,920 --> 00:16:25,920
然後當你

342
00:16:25,920 --> 00:16:27,519
然後你可以看到那裡的權重

343
00:16:27,519 --> 00:16:32,880
是的，你可以將原始輸入定義為全為一，以觀察權重的分佈情況

344
00:16:32,880 --> 00:16:34,480
然後你可以，那是一個，那是一個

345
00:16:34,480 --> 00:16:38,240
那只是一個獨立的講座，所以現在你會知道

346
00:16:38,240 --> 00:16:42,560
這些分佈的重點，你知道的就在那裡

347
00:16:42,560 --> 00:16:42,959
好的

348
00:16:42,959 --> 00:16:44,240
所以我

349
00:16:44,240 --> 00:16:49,519
至少希望你知道在這種情況下內核初始化器在做什麼，但更重要的是

350
00:16:49,519 --> 00:16:52,320
什麼是上採樣

351
00:16:52,320 --> 00:16:53,279
和

352
00:16:53,279 --> 00:16:57,600
對不起，讓我們執行這個，這樣我們可以查看這裡的相同內容

353
00:16:57,600 --> 00:16:58,079
好的

354
00:16:58,079 --> 00:17:00,800
至少現在你知道升頻的區別了

355
00:17:00,800 --> 00:17:04,319
和可轉置地正常處理

356
00:17:04,319 --> 00:17:06,559
我 我

357
00:17:06,559 --> 00:17:08,400
我差點說我會用完樣本

358
00:17:08,400 --> 00:17:11,679
但最近我一直在使用 con 2d 轉置

359
00:17:11,679 --> 00:17:17,359
而且我沒有看到結果上有任何明顯的差異

360
00:17:17,359 --> 00:17:18,240
開啟 開啟

361
00:17:18,240 --> 00:17:22,160
我所處理的一小部分數據集上，但至少這是重要的

362
00:17:22,160 --> 00:17:26,160
讓你知道差異在哪裡，我希望這段視頻能達成這個任務

363
00:17:26,160 --> 00:17:27,919
請繼續訂閱這段視頻

364
00:17:27,919 --> 00:17:30,320
在下一個部分讓我們繼續這些討論

365
00:17:30,320 --> 00:17:36,000
這樣最終我們積累足夠的知識以進行二元語義分割

366
00:17:36,000 --> 00:17:37,120
和多類別

367
00:17:37,120 --> 00:17:39,039
和 3D 多類別

368
00:17:39,039 --> 00:17:40,160
數據集，謝謝大家

